# 7 Observability with Istio: understanding the behavior of your services

DevOps나 SRE에서 핵심적으로 다룰 정도로 운영 측면에서 Observability는 상당히 중요해졌다.
운영 중인 서비스에서 어떤 일이 일어나고 있는지 살펴보고, 장애 상황을 빠르게 인지하고 복구하거나 개선시켜 나가므로써 결국 서비스의 품질을 향상시킬 수 있다.
하지만 마이크로 서비스에서는 하드웨어 서버가 아닌 더 작은 단위의 인스턴스들로 구성되어 있으며, 관리해야할 대상은 작게는 수십, 많게는 수천, 수만이 될지도 모른다.
이러한 상황에서 Obserbavility를 확보하는 것 자체도 굉장히 어려운 것인데, Istio가 Obserbavility를 위해 어떤 magic을 부렸는지 살펴보자.


### 7.1.1  Observability vs Monitoring
Monitoring은 Obervability의 subset이다. 
metric들을 모아 aggregating해서 어떤 treshold를 넘기면 alert을 보내는 데 이것이 바로 Monitoring 이라고 볼 수 있다. 

만약 어떤 유저에게 느린 응답을 내려갔을 때, 에러가 발생한 것도 아니고 alert이 걸려있지 않았다면 관리자는 인지할 수 없을 것이다.
하지만 obsavability를 고려해서 디자인을 한다면 service latency, intrastructure hops 같은 메트릭을 수집하고, 이러한 데이터를 통해서 에러가 아니더라도 서비스에 개선에 필요한 포인트들을 가시화할 수 있게된다.


### 7.1.2  How Istio helps with observability
Istio는 Obserbavility를 위해서 좋은 위치를 선점하고 있다. 바로 Sidecar proxy인 Envoy에서 서비스로 들어오는 혹은 나가는 트래픽을 처리할때 발생하는 여러 메트릭들을 애플리케이션 대신에 모을 수 있다. Prometheus나 Grafana에서 확인할 수 있는 모니터링에 필요한 기본적인 latency나 error count 뿐 아니라, OpenTracing 지원을 통해서 distributed tracing 데이터도 생성할 수 있으며 Jeager나 Zipkin을 통해서 확인이 가능하다.


## 7.2  Collecting metrics from Istio data plane
이 책의 소스는 (여기|https://github.com/istioinaction/book-source-code)에 있는 것을 사용하며, 7장(Observability) 내용을 실습하기 위해서는 `work-on-chapter06` 브랜치로 switch 해야한다.

아래 명령을 통해서 `catalog` 라는 서비스를 배포한다.
여기에는 `catalog`라는 서비스(혹은 애플리케이션) 자체 뿐 아니라, kubernetes의 `service, deployment`, istio의 `gateway, virtual service` 와 같은 리소스의 생성도 포함되어 있다.
```bash
make deploy-apigateway-with-catalog
```

이제 아래 명령을 통해서 서비스에 접근해서 정상적으로 실행되는지 살펴보자.
(참고로 ingress에 접근하기 위한 url을 뽑아오는 것도 makefile에 포함되어 있음)
```bash
curl  -H "Host: apigateway.istioinaction.io"  $(make ingress-url)/api/products
```

apigateway로 들어오는 incoming traffic과 관련된 통계 정보들을 살펴보면 
```bash
 kubectl exec -it $(make apigateway-pod) -c istio-proxy -- curl localhost:15000/stats | grep cluster.local | grep 8080
```
이 서비스에서 3개의 request가 성공했으며 2xx, 200을 리턴했다고 나오며, 아래와 같은 메트릭들을 StatsD와 같은 metric-collection system에 연동할 수 있다. 
```
apigateway.istioinaction.svc.cluster.local.internal.upstream_rq_200: 3
apigateway.istioinaction.svc.cluster.local.internal.upstream_rq_2xx: 3
apigateway.istioinaction.svc.cluster.local.internal.upstream_rq_completed: 3
...
```


### 7.2.1  Pushing Istio metrics into statsD

metric을 전송하기 위해서 StatsD를 setup해야한다. 
```bash
kubectl apply -f chapters/chapter7/statsd
```

StatsD 컨테이너로 들어가서 메트릭을 조회해보자.
```bash
kubectl exec -it pod_name_of_statsd sh
/app/statsd # echo "counters" | nc 127.0.0.1 8126
```

Istio가 StatsD 서버로 메트릭을 전송할 수 있게 deployment를 수정해보자.
아래는 in-place로 kubernetes 스펙을 수정할 수 있는 명령인 `kubectl edit`을 사용했으며 이때 Editor로 `vim`을 사용하였다.
```bash
KUBE_EDITOR="vim" kubectl edit deploy apigateway
```

수정해야할 부분은 apigateway의 sidecar proxy 컨테이너를 설정하는 부분의 실행 옵션에 `--statsdUdpAddress`를 추가하는 것이다.
```yaml
- args:
    - proxy
    - sidecar
    - --statsdUdpAddress
    - statsd:8125
```

이제 apigateway pod에 대한 Obserbavility가 확보되었다.
위에서 실행했던 StatsD의 메트릭을 조회하는 명령을 다시 실행해보면, envoy에서 전송된 메트릭들이 추가된 것을 확인할 수 있을 것이다.
```bash
kubectl exec -it pod_name_of_statsd sh
/app/statsd # echo "counters" | nc 127.0.0.1 8126
```

이번에는 모든 sidecar proxy가 우리가 생성한 StatsD로 metric을 전송하게 해보자.
Istio에서 sidecar proxy를 주입시켜줄때 참조하는 스펙을 변경하는 것이다.
위에서 했던것처럼 `--statsdUdpAddress` parameter를 추가하면된다. 
```bash
KUBE_EDITOR="vim" kubectl edit cm/istio-sidecar-injector -n istio-system
```

### 7.2.2  Pulling Istio Metrics into Prometheus
이제 Prometheus로 메트릭을 collect해보자. 위에서의 메트릭을 StatsD로 전송을 해야했지만, Prometheus는 기본적으로 metric의 endpoint로부터 pulling하기 때문에 조금 다른 설정이 필요하다.

기본적으로 Prometheus에서 pulling할 endpoint(localhost:15090/stats/prometheus)를 제공해주며, 이 endpoint를 통해서 수집되는 metric을 살펴보자.
```bash
kubectl exec -it pod_name_of_apigateway curl localhost:15090/stats/prometheus
```

Prometheus에서는 이러한 metric을 scrap하도록 설정만 하면 된다.
```yaml
# prometheus.yaml

global:
  scrape_interval: 15s
scrape_configs:

# Scrape config for envoy stats
- job_name: 'envoy-stats'
  metrics_path: /stats/prometheus	# scrape할 path
  kubernetes_sd_configs:			# kubernetes API

...

relabel_configs:
  - source_labels: [__meta_kubernetes_pod_container_port_name]
    action: keep
    regex: '.*-envoy-prom'			# prometheus port를 찾는다
```

그리고 이러한 설정 파일(prometheus.yaml)의 경우는 kuberntes의 ConfigMap로 설정하고 해당 pod에서 volume mount하는 방식으로 사용할 수 있다.
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    source: istioinaction
  name: istioinaction-prom
data:
  prometheus.yml: |-
    global:
      scrape_interval: 15s
    scrape_configs:

    # Scrape config for envoy stats
    - job_name: 'envoy-stats'
      metrics_path: /stats/prometheus
      kubernetes_sd_configs:
      - role: pod

      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_container_port_name]
        action: keep
        regex: '.*-envoy-prom'
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:15090
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: pod_name
```
```bash
kubectl create -f chapters/chapter7/prom/prometheus-configmap.yaml
```

Prometheus의 Deployement에서 configMap을 volume mount로 사용할 수 있으며, Prometheus 실행시 parameter로 이 설정 파일을 명시해준다.
```yaml
apiVersion: extensions/v1beta1
kind: Deployment
...
  template:
  ...
    spec:
      containers:
      - args:
        - --storage.tsdb.retention=6h
        - --config.file=/etc/prometheus/prometheus.yml
        image: docker.io/prom/prometheus:v2.3.1
        imagePullPolicy: IfNotPresent
        ...
        volumeMounts:
        - mountPath: /etc/prometheus
          name: config-volume
      ...
      serviceAccount: prometheus
      serviceAccountName: prometheus
      volumes:
      - configMap:
          name: istioinaction-prom
        name: config-volume

```

Prometheus configuration을 생성했으니, deployment도 생성해보자.
```bash
kubectl create -f chapters/chapter7/prom/prometheus-deployment.yaml
```


이제 새로운 prometheus 서버로 쿼리해보자. 테스트를 위해 local환경에서 Prometheus에 접근하기 위해서 port-forward를 실행해보자.
```bash
kubectl port-forward pod_name_of_prometheus 9090:9090
```
이제 http://localhost:9090/graph 에 접속해 볼 수 있다.



## 7.3  Creating new metrics to send to Prometheus through Istio-telemetry
새로운 metric을 추가하기 위해서는 아래와 같은 단계가 필요하다.
1. Istio에서 제공하는 `metric` 이라는 spec통해서 어떤 내용의 metric을 담을지 설정한다.
2. 추가한 metric이 어디로 가야하는지 Istio에게 알려준다.
3. metric 생성의 기준을 정의한다.

일단 `metric`이라는 spec을 통해서 신규 metric에 대해서 설정해준다. 
아래에서는 `dimensions` 부분에 추가되는 metric 정보가 담겨져 있으며, count 정보이기 때문에 value에 `1`이라는 값을 할당했다.
```yaml
apiVersion: "config.istio.io/v1alpha2"
kind: metric
metadata:
  name: apigatewayrequestcount
  namespace: istio-system
spec:
  value: "1"
  dimensions:
    source: source.workload.name | "unknown"
    destination: destination.workload.name | "unknown"
    destination_ip: destination.ip
  monitored_resource_type: '"UNSPECIFIED"'
```

이러한 metric을 실제 prometheus에서 어떻게 수집될지 아래와 같이 설정할 수 있는데, 이것은 Istio에서 제공하는 Prometheus를 위한 핸들러이다.
신규 metric을 수집할 위치(apigatewayrequestcount.metric.istio-system)와 어떤 metric(label_names)을 수집할지, 어떤 타입의 metric인지(COUNTER) 설정했다.
```yaml
apiVersion: "config.istio.io/v1alpha2"
kind: prometheus
metadata:
  name: apigatewayrequestcounthandler
  namespace: istio-system
spec:
  metrics:
    - name: apigateway_request_count
      instance_name: apigatewayrequestcount.metric.istio-system
      kind: COUNTER
      label_names:
        - source
        - destination
        - destination_ip
```

마지막으로 위에서 생성한 핸들로(apigatewayrequestcounthandler)에서 추가한 metric을 생성하는 condition을 명시한다.
이것 역시 Istio에서 지원해주는 rule이라는 spec을 사용하는데, `match`라는 부분에 조건을 명시할 수 있다.
아래 예제에서는 `destination.service`라는 attribute 값이 `"apigateway.istioinaction.svc.cluster.local"`과 `match`할 경우에 `actions`에 명시된 handler(`apigatewayrequestcounthandler`)의 metric을 발생시키는 것이다.
```bash
apiVersion: "config.istio.io/v1alpha2"
kind: rule
metadata:
  name: apigatewayrequestcountrule
  namespace: istio-system
spec:
  match: destination.service == "apigateway.istioinaction.svc.cluster.local"
  actions:
    - handler: apigatewayrequestcounthandler.prometheus
      instances:
        - apigatewayrequestcount.metric
```

예제에 포함되어 있는 metric 리소스를 생성하자.
```bash
kubectl create -f chapters/chapter7/istio-telemetry/new-metric.yaml
```

그리고 몇개의 reqeust를 전송해서, Prometheus에 새로운 metric이 보이는지 살펴보자.
```bash
curl  -H "Host: apigateway.istioinaction.io" $(make ingress-url)/api/products
```

Port forward를 잊지말자.
```bash
kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath='{.items[0].metadata.name}') 9090:9090
```
그리고 `localhost:9090/graph`에 접근해보면 생성된 metric들을 볼 수 있을것이다.


## 7.4  Distributed tracing with OpenTracing
Istio는 특정 언어의 libraries나 application 필요없이 span을 distributed tracing engine에 전송할 수 있다.
이것도 당연히 sidecar proxy에서 서비스에 대한 트래픽을 처리하는 동작에 관여할 수 있기 때문에 가능한 일이다. 
sidecar proxy는 어떤 parent span을 기준으로 child span을 생성할지, 또 어떤 메트릭들로 구성할지 tracing 처리 작업을 하는데, 이것은 일반적인 tracing 처리와 동일하다.

sidecar proxy에서는 이러한 tracing metric을 전송할 위치를 설정해야한다.
```bash
kubectl  get pod pod_name_of_apigateway -o yaml | grep zipkin
    - --zipkinAddress
    - zipkin.istio-system:9411
```
이 예제에서는 distributed tracing engine으로 `Jeager`를 사용하지만, legacy 이슈로 여전히 `zipkinAddress`로 위치를 설정해줘야한다.


실제 tracing 정보가 전파되는 것을 확인하기 위해서, `VirtualService`를 배포하여 라우팅을 설정하자. 
```bash
kubectl create -f chapters/chapter7/tracing/thin-httpbin-virtualservice.yaml
```

그리고 span을 포함하여 ingress로 요청을 날려보자.
```bash
GATEWAY_RULE=$(make ingress-url)
curl -H "Host: httpbin.istioinaction.io" http://$GATEWAY_URL/headers
{
  "headers": {
    "Accept": "*/*",
    "Host": "httpbin.istioinaction.io",
    "User-Agent": "curl/7.54.0",
    "X-B3-Sampled": "0",
    "X-B3-Spanid": "463e3ca7c53e2c85",
    "X-B3-Traceid": "463e3ca7c53e2c85",
    "X-Envoy-Decorator-Operation": "httpbin.org:80/*",
    "X-Envoy-Internal": "true",
    "X-Istio-Attributes": "CikKGGRlc3RpbmF0aW9uLnNlcnZpY2UuaG9zdBINEgtodHRwYmluLm9yZwopChhkZXN0aW5hdGlvbi5zZXJ2aWNlLm5hbWUSDRILaHR0cGJpbi5vcmcKKgodZGVzdGluYXRpb24uc2VydmljZS5uYW1lc3BhY2USCRIHZGVmYXVsdAokChNkZXN0aW5hdGlvbi5zZXJ2aWNlEg0SC2h0dHBiaW4ub3JnCk8KCnNvdXJjZS51aWQSQRI/a3ViZXJuZXRlczovL2lzdGlvLWluZ3Jlc3NnYXRld2F5LTViNjRmZmZjOWYtenp3bTUuaXN0aW8tc3lzdGVt"
  }
}
```

위의 요청에 의해서 수집된 tracing 메트릭을 확인해보자.
테스트를 위해 local에서 Jeager로 접근이 가능하도록 port-fowrad를 실행하자.
```bash
kubectl port-forward deploy/istio-tracing -n istio-system 8080:16686
```
그리고 http://localhost:8080 로 접속하면 Jaeger UI를 확인할 수 있다.

이러한 tracing 정보의 전송은 모든 reqeust마다 전송이 필요하다면, 그것을 처리하는 파이프라인의 부담을 주게된다.
특히 Istio의 컴포넌트인 Pilot에서 처리하는 부하도 이것에 비례하게 된다. 
따라서 이런 부담을 줄이고자 Tracing 전송에 샘플링을 적용할 수 있으며, pilot의 Deployment에서 `PILOT_TRACE_SAMPLING`라는 환경 변수를 설정하여 간단하게 적용할 수 있다. 
단, 샘플링을 명시적으로 피하고 싶을 경우는 `x-envoy-force-trace`라는 header를 사용하면 된다.
```bash
kubectl -n istio-system edit deploy istio-pilot
```
```yaml

containers:
- args:
  - discovery
  env:
  - name: POD_NAME
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: metadata.name
  - name: POD_NAMESPACE
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: metadata.namespace
  - name: PILOT_CACHE_SQUASH
    value: "5"
  - name: GODEBUG
    value: gctrace=2
  - name: PILOT_PUSH_THROTTLE_COUNT
    value: "100"
  - name: PILOT_TRACE_SAMPLING
    value: "50.0"
```











